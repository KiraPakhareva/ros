# Как построить график результатов тренировки

Построение графика результатов очень важно, потому что вы можете визуально определить, если ваша система учится или нет, и как быстро. Если вы можете заранее определить, что ваша система не учится должным образом, то вы можете изменить параметры \(или даже условия эксперимента\), так что вы можете быстро повторить попытку. Для того, чтобы построить результаты, я предоставил скрипт на Python в каталоге utils, который выполняет эту работу. Я не создавал код сам, я взял его откуда-то еще, но я не могу вспомнить, откуда \(если вы являетесь автором и хотите иметь кредит, просто свяжитесь со мной\). Чтобы запустить этот код, просто введите его в каталог utils:

```text
> python plot_results.py
```

Сценарий будет принимать результаты, полученные в справочнике training\_results, и генерировать сюжет со всеми наградами, полученными за каждый эпизод. Для просмотра графика необходимо открыть окно графические инструменты \(Tools-&gt;Graphic Tools&gt; \). Вы должны увидеть что-то вроде этого:

![](.gitbook/assets/image%20%285%29.png)

Для этого поста я запустил код, предоставленный вам \(как версия 8 

февраля 2018\) для 500 эпизодов, и результаты не очень хороши, как вы можете видеть на следующем рисунке

![](.gitbook/assets/image%20%286%29.png)

Здесь вы можете видеть, что нет никакого прогресса в вознаграждении, эпизод за эпизодом. Кроме того, вариации в значениях вознаграждения выглядят совершенно случайными. Это означает, что алгоритм на самом деле не учится вообще на проблеме, которую пытается решить. Какие могут быть причины для этого? Ну, я могу вычислить несколько. Цель для инженера состоит в том, чтобы разработать способы изменения ситуации обучения, чтобы обучение действительно могло быть выполнено. Некоторые возможные причины, почему это не обучение:

• Функция вознаграждения не создана должным образом. Если вознаграждение является слишком сложным, система может быть не в состоянии захватить маленькие шаги ребенка улучшения.   
• Состояние, предоставляемое алгоритму обучения, является непрерывным, и Qlearning не очень хорошо подходит для этого. Как вы можете видеть в коде, мы возвращаем текущее положение робота по оси x как состояние окружающей среды. Это непрерывное значение. Я бы предложил дискретизировать государство на разные зоны \(скажем, 10 зон\). Каждая зона означает приближение все ближе и ближе к точке цели.   
• Параметры алгоритма обучения не являются корректными.   
• Сам по себе эксперимент не является полностью обоснованным . Примите во внимание, что положение цели робота фиксировано, и что препятствия не могут быть обнаружены роботом \(он может только определить их положение после многократного столкновения с ним в нескольких эпизодах\).

Очевидно, что структура обучающей среды является правильной \(под структурой я подразумеваю организацию всей системы обучения\). Это хороший момент, поскольку он позволяет нам начать искать пути улучшения обучения изнутри структуры обучения, которая уже работает.

